{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5a5acd-e401-4665-a988-c00f97c67527",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c1071-d8b1-4fe7-81e7-cbb9b294e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An activation function in artificial neural networks is a mathematical function applied to each neuron's output. It determines whether a \n",
    "neuron should be activated or not by transforming the input signal of the neuron into an output signal. Activation functions introduce\n",
    "non-linearity into the network, enabling it to learn complex patterns and make accurate predictions.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4bdc6c-8bb8-4150-acde-1d6337884eb3",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f28c46-757f-4b0a-b864-db0aff6ede1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sigmoid: Outputs values between 0 and 1, useful for binary classification.\n",
    "Tanh (Hyperbolic Tangent): Outputs values between -1 and 1, often used in hidden layers.\n",
    "ReLU (Rectified Linear Unit): Outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "Leaky ReLU: A variant of ReLU that allows a small, non-zero gradient when the input is negative.\n",
    "Softmax: Converts a vector of values into a probability distribution, typically used in the output layer for multi-class classification.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afe521-61d2-4640-a574-235c2a9945e7",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41975b57-a70a-447d-ad9c-54093be326f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Activation functions affect the training process and performance of a neural network by:\n",
    "\n",
    "Introducing non-linearity, which allows the network to learn complex patterns.\n",
    "Influencing the gradient flow during backpropagation. Activation functions with vanishing gradients (e.g., sigmoid and tanh) can slow down training.\n",
    "Affecting the convergence speed and stability. Some activation functions (e.g., ReLU) help in faster convergence compared to others.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc2511-d52e-4e94-83ab-eda1a47ab2a7",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f5d6e-3410-4f01-99e4-81e10b2acb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ". It squashes input values to a range between 0 and 1.\n",
    "\n",
    "Advantages: Smooth gradient, outputs can be interpreted as probabilities.\n",
    "Disadvantages: Vanishing gradient problem, which can slow down training and make it difficult for the network to learn.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effa576-5ea4-4143-888d-8a5223654fe9",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012827a-ce90-46d0-83bc-f85d7a15b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The ReLU activation function is defined as ùëì(ùë•)=max(0,ùë•)f(x)=max(0,x). It outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "\n",
    "Differences from Sigmoid: ReLU is not bounded and does not saturate for large positive values, which helps in mitigating the vanishing gradient problem. ReLU is also computationally more efficient.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d667634a-7ab1-47f1-a746-25d5508476c5",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6c6ee-137f-419c-a080-6d56e24fd85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mitigates Vanishing Gradient: ReLU does not saturate in the positive domain, preventing the vanishing gradient problem.\n",
    "Computational Efficiency: ReLU requires less computation compared to sigmoid.\n",
    "Sparse Activation: It leads to sparse activation, where only a subset of neurons are activated, improving computational efficiency.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a7b74-19a6-48e3-b62d-43f21fee20bf",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae4437-22dd-4d85-98dd-718995c8fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Leaky ReLU is a variant of ReLU where the function allows a small, non-zero gradient when the input is negative. It is defined as ùëì(ùë•)=ùë• f(x)=x if ùë•>0x>0 and ùëì(ùë•)=ùõºùë•f(x)=Œ±x if ùë•‚â§0\n",
    "x‚â§0, where \n",
    "Œ± is a small constant.\n",
    "\n",
    "Addressing Vanishing Gradient: By allowing a small gradient when the input is negative, leaky ReLU ensures that neurons do not die and continue to learn, thus addressing the vanishing gradient problem.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52469d41-ba9c-4a2c-a3ff-cd16b56d5669",
   "metadata": {},
   "source": [
    "# Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064a25c-f38d-45ce-b9b9-1575b807208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The softmax activation function converts a vector of values into a probability distribution.  \n",
    "\n",
    "\n",
    "Common Usage: Softmax is commonly used in the output layer of neural networks for multi-class classification problems, where it provides a probabilistic interpretation of the class scores\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7840fe-cded-442f-a35c-635354ecfdc9",
   "metadata": {},
   "source": [
    "# Ans : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e503808-5ed8-4eeb-a7ca-5576b9ff3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
